def plot_roc_auc(model, X_test, y_test):
    
    y_probs = model.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_probs)
    roc_auc = sklearn.metrics.auc(fpr, tpr)

    print(f"ROC AUC Score: {roc_auc}")

    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier (AUC = 0.5)')

    RocCurve = sklearn.metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)
    RocCurve.plot(ax=plt.gca())  

    plt.legend(loc='lower right')
    plt.show()

def plot_feature_importances(model, X_train, figsize, top_features=10):
                                                     
    feature_importances = model.get_feature_importance()
    feature_names = X_train.columns 

    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importances
    }).sort_values(by='Importance', ascending=False)
    

    plt.figure(figsize=figsize)
    plt.barh(feature_importance_df['Feature'][:top_features], feature_importance_df['Importance'][:top_features], color='skyblue')
    plt.gca().invert_yaxis() 
    plt.xlabel('Importance in %')
    plt.ylabel('Feature')
    plt.title(f'Top {top_features} Feature Importances')
    plt.show()

def print_metrics(model, X_test, y_actual, y_predicted):
    
    print("Accuracy:", round(accuracy_score(y_actual, y_predicted), 2))
    print("Precision:", round(precision_score(y_actual, y_predicted), 2))
    print("Recall:", round(recall_score(y_actual, y_predicted), 2))
    print("F1-score:", round(f1_score(y_actual, y_predicted), 2))
    print("ROC-AUC:", round(roc_auc_score(y_actual, model.predict_proba(X_test)[:, 1]), 2))


def cm(y_actual, y_pred):
    
    cm = confusion_matrix(y_actual, y_pred)
    
    fig, axes = plt.subplots(1, 2, figsize=(11, 3))
    
    sns.heatmap(data=cm, annot=cm/cm.sum(), fmt='0.2%', cmap='Blues', ax=axes[0])    
    axes[0].set_title("Relative Confusion Matrix")
    axes[0].set_xlabel("Predicted")
    axes[0].set_ylabel("Actual")

    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[1])
    axes[1].set_title("Absolute Confusion Matrix")
    axes[1].set_xlabel("Predicted")
    axes[1].set_ylabel("Actual")

def prob_distribution(model, X_test):
    
    probs = model.predict_proba(X_test)
    
    fig, axes = plt.subplots(1, 2, figsize=(11, 3))
    
    sns.histplot(probs[:, 1], kde=True, bins=20, ax=axes[0])    
    axes[0].set_title("Probability Distribution for Positive Class")
    axes[0].set_xlabel("Predicted Probability")
    axes[0].set_ylabel("Frequency")

    sns.kdeplot(probs[:, 0], label='Class 0', ax=axes[1])
    sns.kdeplot(probs[:, 1], label='Class 1', ax=axes[1])
    axes[1].set_title("Probability Distributions for All Classes")
    axes[1].set_xlabel("Predicted Probability")
    axes[1].set_ylabel("Density")
    plt.legend()

